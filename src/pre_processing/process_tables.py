"""Replace the tables in the md files generated from minerU with their descriptions.

The markdown files generated after the SEC10K PDF filings were processed by the minerU
library contain html formatted tables that might not be the best for being parsed by
LLM models. Thus, the images of the tables, which were also generated by the minerU
parsing process, along with some context are fed into an LLM, for it to provide a
description of the table. The html table is then replaced with its equivalent plain text
description.
"""

import base64
import json
import logging
from pathlib import Path
from typing import Any, Dict, List

import hydra
from langchain_core.messages import HumanMessage

log = logging.getLogger(__name__)


def get_table_description(cfg, table_image_path, company_name, table_context):
    r"""Returns a string description of a table given the image path and the context.

    In the markdown file, the tables are encoded in html and all have newlines before
    and after them. The context for these tables is the few lines of text preceeding
    the table in the markdown file, along with the table caption and table footnote
    present in the content list file. The minerU library also produces images of
    tables, and the path to these images is present in the content list file.

    All this context, along with the image are passed to an LLM to obtain a string
    description of the table. The description contains both a summary of the table
    and a row by row description of its contents. Since we need an LLM that handles
    both images and text, the Gemma-3 27B model was used.

    Args:
        cfg (Any): hydra configuration object. Should have the following attributes:
            `cfg.models.table_descriptions` (used to instantiate the LLM provider from
            langchain), `cfg.models.table_descriptions.name` (name of the model to use
            from that provider), `cfg.prompts.table_description` (the LLM prompt to get
            the table description from the LLM)
        table_image_path (Path): A path object with the path to the image of the table
            whose description is required.
        company_name (str): The ticker symbol of the company, while parsing whose
            markdown file, the table in question was obtained.
        table_context (str): A string containing the table caption and table footnote
            (if they were provided) from the contents_list.json file for the company,
            along with some sentences from just before the table, obtained from the
            markdown file for the company.

    Returns:
        str: The summary and description of the table obtained from the LLM. It should
            start with a summary of a few lines and be followed by a row-by-row
            description of the table.
    """
    provider = hydra.utils.instantiate(cfg.models.table_descriptions)
    model = provider(model=cfg.models.table_descriptions.name)
    # get image as base64
    with open(table_image_path, "rb") as file:
        base64_img = base64.b64encode(file.read()).decode("utf-8")
    message = HumanMessage(
        content=[
            {
                "type": "text",
                "text": cfg.prompts.table_description.format(
                    company_name=company_name, table_context=table_context
                ),
            },
            {
                "type": "image_url",
                "image_url": {"url": f"data:image/jpeg;base64,{base64_img}"},
            },
        ]
    )
    table_description = model.invoke([message]).content.replace("\n", " ")
    return table_description


def process_tables(cfg: Any) -> None:
    r"""Fill json file with LLM-derived descriptions of all tables for all companies.

    The function performs the following operations:
    1. Parses markdown files, splitting content by double newlines.
    2. Validates consistency between markdown tables and a content list JSON.
    3. Extracts context (preceding text) for each table.
    4. Queries an LLM for a summary (via `get_table_description`).
    5. Incrementally saves results to a JSON file to support resume-on-failure.
    """
    for ticker in cfg.data.companies:
        _process_company_tables(cfg, ticker)


def _process_company_tables(cfg: Any, ticker: str) -> None:
    """Fill json file with LLM-derived descriptions of all tables for specific company.

    We intersperse the LLM calls with json file read-writes to add a bit of lag. We
    also allow for resumption - don't have to do everything in one go.
    """
    ticker_lower = ticker.lower()

    # resolve file paths
    md_path = Path(cfg.pre_proc.md_file_path.format(ticker=ticker_lower))
    content_list_path = Path(
        cfg.pre_proc.content_list_file_path.format(ticker=ticker_lower)
    )
    images_folder = cfg.pre_proc.images_folder_path.format(ticker=ticker_lower)
    json_descriptions_path = Path(cfg.pre_proc.table_descriptions_path)

    # load and prepare data
    split_text = _read_and_clean_markdown(md_path)
    content_list = _load_json(content_list_path, fail_on_error=True)
    json_descriptions = _load_json(json_descriptions_path, fail_on_error=False)

    # validate data integrity
    _validate_table_consistency(cfg, ticker, split_text, content_list)

    # map table body to its content metadata for o(1) access
    content_list_table_lookup = {
        c["table_body"]: c
        for c in content_list
        if c.get("type") == "table" and "table_body" in c
    }
    json_descr_table_lookup = {
        (descr["Ticker"], descr["Split Index"]): descr
        for descr in json_descriptions
        if descr.get("Table Description", "").strip()
    }

    # iterate through markdown splits to find and process tables
    for idx, segment in enumerate(split_text):
        # skip non-table segments
        if cfg.pre_proc.table_start not in segment:
            continue
        # skip if we have already obtained the summary of this table
        json_descr = json_descr_table_lookup.get((ticker, idx), "Data")
        if not json_descr:
            continue
        # retrieve metadata
        content_data = content_list_table_lookup.get(segment)
        if not content_data:
            continue

        # extract components
        caption = " ".join(content_data.get("table_caption", [])).strip()
        footnote = " ".join(content_data.get("table_footnote", [])).strip()
        img_filename = content_data["img_path"].split("/")[1]
        image_path = Path(f"{images_folder}{img_filename}")

        # build context string
        context_text = _extract_context(cfg, split_text, idx)
        full_context = _format_context_string(caption, footnote, context_text)

        # get description from llm
        table_description = get_table_description(cfg, image_path, ticker, full_context)

        # Persist result immediately
        result_entry = {
            "Image": str(image_path),
            "Ticker": ticker,
            "Table Context": full_context,
            "Split Index": idx,
            "Split": segment,
            "Table Description": table_description,
        }
        _append_result_to_file(cfg.pre_proc.table_descriptions_path, result_entry)


def _read_and_clean_markdown(path: Path) -> List[str]:
    """Read markdown file and split by double newlines, trimming whitespace."""
    with open(path, "r", encoding="utf-8") as f:
        # split by double newline to separate paragraphs/tables
        splits = f.read().split("\n\n")
    return [s.strip() for s in splits]


def _load_json(path: Path, fail_on_error: bool = False) -> List[Dict[str, Any]]:
    """Load JSON content from a file."""
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError) as e:
        if fail_on_error:
            log.error(f"Error while loading json: {repr(e)}.\nFailing.")
            raise Exception(f"Error while loading json file with path {path}")
        else:
            log.error(f"Error while loading json: {repr(e)}\nContinuing anyway.")


def _validate_table_consistency(
    cfg: Any, ticker: str, splits: List[str], content_list: List[Dict[str, Any]]
) -> None:
    """Ensure bijection between tables from markdown file and from content_list.json.

    We validate the following:
    1. Every split from the markdown file that contains a table, only contains a table.
    2. Ensure every valid table type element in the contents_list.json file contains
       the table body (which is the actual text found in the markdown file) and the
       image path for the table. We only consider those tables to be valid which have a
       table_body key (there are some without it).
    3. Ensure that the number of tables found from the contents list file are the same
       as the number of tables found in the markdown file.
    """
    # ensure that every split from the markdown file that contains a table, only has
    # a table in it
    for split in splits:
        if cfg.pre_proc.table_start in split or cfg.pre_proc.table_end in split:
            assert split.startswith(cfg.pre_proc.table_start), "Malformed table start"
            assert split.endswith(cfg.pre_proc.table_end), "Malformed table end"
    # count tables in markdown
    num_tables_md = sum(1 for s in splits if cfg.pre_proc.table_start in s)

    # validate content list tables
    num_content_tables = 0
    for content in content_list:
        if content.get("type") == "table" and "table_body" in content:
            num_content_tables += 1
            assert content["table_body"] in splits, "Content list table not found in MD"
            assert content.get("img_path", "").strip(), "No image path in content list"

    # ensure content_list and markdown file have the same number of tables
    assert num_content_tables == num_tables_md, (
        f"Ticker: {ticker}. Number of tables obtained from the content_list.json"
        f"file {num_content_tables} is different from the number of tables obtained"
        f"from the splitting of the markdown file {num_tables_md}."
    )


def _extract_context(cfg, splits: List[str], current_idx: int) -> str:
    """Extract text from the previous N non-table splits, stopping at headers."""
    context_parts = []
    splits_obtained = 0
    # iterate backwards from current table
    for i in range(current_idx - 1, -1, -1):
        segment = splits[i]
        # skip other tables
        if cfg.pre_proc.table_start in segment:
            continue
        if splits_obtained >= cfg.pre_proc.num_splits_table_context:
            break
        # if header is encountered, add it and stop
        if segment.startswith("#"):
            context_parts.append(segment)
            break
        context_parts.append(segment)
        splits_obtained += 1
    # reverse to restore original order (closest context last)
    return " ".join(context_parts[::-1]).strip()


def _format_context_string(caption: str, footnote: str, text_context: str) -> str:
    """Format the final context string for the LLM."""
    parts = []
    if caption:
        parts.append(f"Caption: {caption}")
    if footnote:
        parts.append(f"Footnote: {footnote}")
    if text_context:
        parts.append(f"Text before the table in the filing: {text_context}")
    return "\n".join(parts)


def _append_result_to_file(file_path: str, entry: Dict[str, Any]) -> None:
    """Append a single record to the JSON list file.

    This is inefficient (Read-Modify-Write) but is robust to crashes and gives some
    time between successive LLM calls to prevent exceeding the rate limit
    """
    path = Path(file_path)
    try:
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        data = []

    data.append(entry)

    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f)


def populate_markdown(cfg):
    r"""Replace the tables in the markdown files with the downloaded descriptions.

    After getting the descriptions of the tables from the LLM and storing them in a
    json file, this function will read the json file and use it to replace the tables
    in the markdown file with their descriptions.
    """
    json_descriptions_path = Path(cfg.pre_proc.table_descriptions_path)
    json_descriptions = _load_json(json_descriptions_path, fail_on_error=True)
    table_lookup = {
        (descr["Ticker"], descr["Index"]): descr
        for descr in json_descriptions
        if descr.get("Table Description", "").strip()
    }
    # for each company, generate splits again, iterate through them, replacing table
    # with their descriptions, join the new splits, write the splits to file
    for ticker in cfg.data.companies:
        # load md file and generate splits
        md_path = Path(cfg.pre_proc.md_file_path.format(ticker=ticker.lower()))
        proc_md_path = Path(
            cfg.pre_proc.processed_md_file_path.format(ticker=ticker.lower())
        )
        split_text = _read_and_clean_markdown(md_path)
        proc_split_text = [
            table_lookup[(ticker, idx)] if cfg.pre_proc.table_start in split else split
            for idx, split in enumerate(split_text)
        ]
        proc_md_file = "\n\n".join(proc_split_text)
        with open(proc_md_path, "w") as file:
            file.write(proc_md_file)
        log.debug(
            f"Created and written new markdown file for {ticker} by replacing"
            "tables with their LLM-derived descriptions"
        )


if __name__ == "__main__":
    import hydra
    from dotenv import load_dotenv
    from omegaconf import DictConfig

    load_dotenv()
    with hydra.initialize(version_base=None, config_path="../../config/"):
        cfg: DictConfig = hydra.compose(
            config_name="config", overrides=[], return_hydra_config=True
        )
    hydra.core.utils.configure_log(cfg.hydra.job_logging, cfg.hydra.verbose)
    process_tables(cfg)
    populate_markdown(cfg)
